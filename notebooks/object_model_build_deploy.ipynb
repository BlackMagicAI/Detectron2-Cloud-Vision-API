{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHnVupBBn9eR"
   },
   "source": [
    "# Black Magic AI Detectron2 Object Detection Cloud Vision API Tutorial\n",
    "\n",
    "<img src=\"../images/blackmagicailogo.png\">\n",
    "\n",
    "This tutorial demonstrats how to create an AWS Detectron2 Object detection Cloud API by deploying a pre-trained  Detectron2 model to an AWS Sagemaker endpoint and exposing it as a REST API using AWS API Gateway.\n",
    "\n",
    "You can make a copy of this tutorial by \"File -> Open in playground mode\" and make changes there. __DO NOT__ request access to this tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM54r6jlKTII"
   },
   "source": [
    "## 1. Install detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FsePPpwZSmqt"
   },
   "outputs": [],
   "source": [
    "# Versions: https://github.com/pytorch/vision/\n",
    "# This is the current pytorch version on Colab. Uncomment this if Colab changes its pytorch version\n",
    "!pip install torch==1.10.2+cu113 torchvision==0.11.3+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html    \n",
    "# Install detectron2 that matches the above pytorch version\n",
    "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
    "!pip install detectron2==0.6 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu113/torch1.10/index.html #commented\n",
    "exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyAvNCJMmvFF"
   },
   "outputs": [],
   "source": [
    "# import some common libraries\n",
    "import numpy as np\n",
    "import cv2, json\n",
    "import torch, torchvision\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.modeling import build_model\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.checkpoint import DetectionCheckpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_FzH13EjseR"
   },
   "outputs": [],
   "source": [
    "# check pytorch installation: \n",
    "import sys\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "print(torchvision.__version__)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk4gID50K03a"
   },
   "source": [
    "## 2. Run a pre-trained Detectron2 Object Detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgKyUL4pngvE"
   },
   "source": [
    "**Define source Image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dq9GY37ml1kr"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "image_filename=\"city-scene.jpg\"\n",
    "input_image=\"../images/\" + image_filename\n",
    "\n",
    "image_src = Image.open(input_image)\n",
    "np_image = np.array(image_src, dtype='float32')\n",
    "\n",
    "image_src.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM1thbN-ntjI"
   },
   "source": [
    "**Then, we create a detectron2 config and a detectron2 `DefaultPredictor` to run inference on this image.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUjkwRsOn1O0"
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "cfg = get_cfg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzJPkUD9mrrN"
   },
   "source": [
    "## 3. Build Object Detection Model\n",
    "[\"...object detection, where the goal is to classify individual objects and localize them using a bounding box...\"](https://kharshit.github.io/blog/2019/08/23/quick-intro-to-instance-segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define custom object detection visualizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_visualizer(image_src, predictions, classes, api=False):\n",
    "    # Draw boxes\n",
    "    # predictions\n",
    "    if (api):\n",
    "        boxes = predictions[0]['pred_boxes']\n",
    "        pred_classes = predictions[0]['pred_classes']\n",
    "        scores = predictions[0]['scores']        \n",
    "    else:               \n",
    "        boxes = predictions[0]['instances'].pred_boxes\n",
    "        pred_classes =predictions[0]['instances'].pred_classes\n",
    "        scores = predictions[0]['instances'].scores\n",
    "\n",
    "    font = ImageFont.truetype('FreeSerif.ttf', 8)\n",
    "    draw = ImageDraw.Draw(image_src)\n",
    "    for box, cl, score in zip(boxes,pred_classes, scores): \n",
    "        text = f\"{classes[cl]} {score:.0%}\"\n",
    "        len = draw.textlength(text=text)\n",
    "        bbox = draw.textbbox((box[0], box[1]), text, font=font)\n",
    "        h = bbox[3] - bbox[1]\n",
    "   \n",
    "        draw.rectangle([(box[0], box[1]-h), (box[0] + len, box[1])], fill=(0,0,0))#text background rectangle\n",
    "        draw.text((box[0], box[1]-h), text, fill=(255, 255, 255))\n",
    "        draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=(0,255,0))#blue rectangle\n",
    "    return image_src\n",
    "\n",
    "all_classed_list = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZEqSBV8RmwRZ"
   },
   "outputs": [],
   "source": [
    "# Object Detection\n",
    "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set https://object-detection-1.notebook.us-east-2.sagemaker.aws/notebooks/Detectron2_Model_Build_Deploy.ipynb#threshold for this model\n",
    "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\")\n",
    "model = build_model(cfg)\n",
    "checkpointer = DetectionCheckpointer(model)\n",
    "checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Default Predictor and Visualizer to validate Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1rOeiKZnGr4"
   },
   "outputs": [],
   "source": [
    "# Object Detection Visualizer\n",
    "predictor = DefaultPredictor(cfg) # normal operation\n",
    "predictions = predictor(np_image)[\"instances\"]\n",
    "# We can use `Visualizer` to draw the predictions on the image.\n",
    "v = Visualizer(np_image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "out = v.draw_instance_predictions(predictions.to(\"cpu\"))\n",
    "plt.imshow(cv2.cvtColor(out.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export mdoel\n",
    "# Ref: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "# https://pytorch.org/vision/stable/index.html\n",
    "# https://pytorch.org/hub/\n",
    "# https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md\n",
    "# tar cmd: tar -czvf model.tar.gz model\n",
    "# tar -czvf model.tar.gz model.pth code\n",
    "torch.save(model, \"models/model-object.pth\", _use_new_zipfile_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create model asset required for Sagemaker endpoint deployment. Copy model.tar.gz to S3 bucket model folder.\n",
    "# Rename model-object.pth file name to model-object.pth per required by Sagemaker endpoint specs\n",
    "tar --transform='flags=r;s|models/model-object.pth|model.pth|' -czvf models/model.tar-object.gz models/model-object.pth code/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Exported Model and Validate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_object_model = torch.load(\"models/model-object.pth\")\n",
    "saved_object_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model experiment without Detectron2 predictor\n",
    "original_image = cv2.imread(input_image) \n",
    "aug = T.ResizeShortestEdge(\n",
    "             [800, 800], 1333\n",
    "#             [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    "        )\n",
    "with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n",
    "        height, width = original_image.shape[:2]\n",
    "        image = aug.get_transform(original_image).apply_image(original_image)\n",
    "        image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "        inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "        predictions = saved_object_model([inputs])\n",
    "#         print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Use Detectron2 Visualizer on saved model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use `Visualizer` to draw the predictions on the image.\n",
    "prediction_output=predictions[0][\"instances\"].to(\"cpu\")\n",
    "v = Visualizer(np_image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "out = v.draw_instance_predictions(prediction_output)\n",
    "plt.imshow(cv2.cvtColor(out.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display results using custom visualizer which does not use Detectron2 dependances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_out = object_visualizer(image_src, predictions, all_classed_list, False)\n",
    "im_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deploy Object Detection Model to Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upload model.tar.gz file to s3 bucket model folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker import get_execution_role, Session\n",
    "\n",
    "sess = Session(default_bucket='<INSERT-AWS-S3-BUCKET-NAME-HERE>')\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "# Connect to S3 bucket and upload file to s3 bucket\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket('<INSERT-AWS-S3-BUCKET-NAME-HERE>').upload_file(\"models/model.tar-object.gz\", \"model/model.tar.gz\")\n",
    "\n",
    "uri = sess.list_s3_files(sess.default_bucket(), 'model')\n",
    "model_data = sagemaker.s3.s3_path_join('s3://', sess.default_bucket(), uri[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Pytorch model and deploy to SageMaker endpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object detection\n",
    "region = sess.boto_region_name\n",
    "serve_image_uri = f\"<INSERT-AWS-ELASTIC-CONTAINER-REGISTRY-REPOSITORY-NAME-HERE>\" ##custom image\n",
    "\n",
    "pyModel = PyTorchModel(\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=\"code\",\n",
    "    role=role,\n",
    "    model_data=model_data,\n",
    "    image_uri=serve_image_uri,\n",
    "    framework_version=\"1.10.2\",\n",
    "    py_version=\"py38\"\n",
    ")\n",
    "\n",
    "predictorEndpt = pyModel.deploy(instance_type='ml.p3.2xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validate Endpoint - perform inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref:\n",
    "# https://aws.amazon.com/blogs/compute/handling-binary-data-using-amazon-api-gateway-http-apis/\n",
    "# https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-payload-encodings.html\n",
    "# https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-binary-data-lambda/\n",
    "# https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-payload-encodings-configure-with-control-service-api.html\n",
    "# https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-payload-encodings-configure-with-console.html\n",
    "# https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-mapping-template-reference.html\n",
    "import boto3\n",
    "import io\n",
    "from base64 import b64encode,b64decode\n",
    "from io     import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "endpoint = '<INSERT_ENDPOINT_NAME_HERE>'\n",
    "runtime= boto3.client('runtime.sagemaker')\n",
    "\n",
    "imgByteArr = io.BytesIO()\n",
    "\n",
    "image_src.save(imgByteArr, format=image_src.format)\n",
    "imgByteArr = imgByteArr.getvalue()\n",
    "\n",
    "# Send image via InvokeEndpoint API\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint, ContentType='application/x-image', Body=imgByteArr)\n",
    "result = response['Body'].read().decode()\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display Detectron2 Object Detections inference results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = json.loads(result) # convert json string to Python dict for parsing\n",
    "im_out = object_visualizer(image_src, res, all_classed_list, True)\n",
    "im_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Call Object Dection API using Python Request library\n",
    "\n",
    "**Create AWS API gateway before performing this step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some common libraries\n",
    "# Using Python Request library\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import io\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Define Constants\n",
    "API_INVOKE_URL='<INSERT_API_INVOKE_URL_HERE>'\n",
    "\n",
    "# define variables\n",
    "url=API_INVOKE_URL\n",
    "\n",
    "def cloud_api_predict(headers, payload):\n",
    "    # send POST request to url\n",
    "    return requests.request(\"POST\", url, headers=headers, data=payload).text\n",
    "\n",
    "# Read image into memory\n",
    "with open(input_image, 'rb') as f:\n",
    "    payload = f.read()\n",
    "\n",
    "headers = {\n",
    "  'Accept': 'image/jpeg',\n",
    "  'Content-Type': 'image/jpeg'\n",
    "}\n",
    "\n",
    "predictions=cloud_api_predict(headers, payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display Detectron2 Object Detections inference results from API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = json.loads(predictions) # convert json string to Python dict for parsing\n",
    "im_out = object_visualizer(image_src, res, all_classed_list, True)\n",
    "im_out.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Detectron2 Tutorial Demo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
