{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHnVupBBn9eR"
   },
   "source": [
    "# Black Magic AI Detectron2 Instance Segmentation Cloud Vision API Tutorial\n",
    "\n",
    "<img src=\"../images/blackmagicailogo.png\">\n",
    "\n",
    "This tutorial demonstrats how to create an AWS Detectron2 Instance Segmentation Cloud API by deploying a pre-trained  Detectron2 model to an AWS Sagemaker endpoint and exposing it as a REST API using AWS API Gateway.\n",
    "\n",
    "You can make a copy of this tutorial by \"File -> Open in playground mode\" and make changes there. __DO NOT__ request access to this tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM54r6jlKTII"
   },
   "source": [
    "# Install detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FsePPpwZSmqt"
   },
   "outputs": [],
   "source": [
    "# Versions: https://github.com/pytorch/vision/\n",
    "# This is the current pytorch version on Colab. Uncomment this if Colab changes its pytorch version\n",
    "!pip install torch==1.10.2+cu113 torchvision==0.11.3+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html    \n",
    "# Install detectron2 that matches the above pytorch version\n",
    "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n",
    "!pip install detectron2==0.6 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu113/torch1.10/index.html #commented\n",
    "exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyAvNCJMmvFF"
   },
   "outputs": [],
   "source": [
    "# import some common libraries\n",
    "import numpy as np\n",
    "import cv2, json\n",
    "import torch, torchvision\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.modeling import build_model\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.checkpoint import DetectionCheckpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_FzH13EjseR"
   },
   "outputs": [],
   "source": [
    "# check pytorch installation: \n",
    "import sys\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "print(torchvision.__version__)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFH0xTpmmlr2"
   },
   "source": [
    "# Run a pre-trained Detectron2 Instance Segmentation model\n",
    "[\"...instance segmentation, we care about detection and segmentation of the instances of objects separately\"](https://kharshit.github.io/blog/2019/08/23/quick-intro-to-instance-segmentation)\n",
    "\n",
    "In other words, we perform segmentation only on the objects detected within the bounding box of object detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgKyUL4pngvE"
   },
   "source": [
    "Define source Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dq9GY37ml1kr"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "input_image=\"../images/\" + \"city-scene.jpg\"\n",
    "image_src = Image.open(input_image)\n",
    "np_image = np.array(image_src, dtype='float32')\n",
    "\n",
    "image_src.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM1thbN-ntjI"
   },
   "source": [
    "Then, we create a detectron2 config and a detectron2 `DefaultPredictor` to run inference on this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUjkwRsOn1O0"
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "cfg = get_cfg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMVmdjzTktjS"
   },
   "outputs": [],
   "source": [
    "# Instance Segmentation\n",
    "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "modelSeg = build_model(cfg)\n",
    "checkpointer = DetectionCheckpointer(modelSeg)\n",
    "checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "modelSeg.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UODpoYClhZl"
   },
   "outputs": [],
   "source": [
    "# Instance Segmentation Visualizer\n",
    "predictor = DefaultPredictor(cfg)\n",
    "predictions = predictor(np_image)[\"instances\"]\n",
    "# We can use `Visualizer` to draw the predictions on the image.\n",
    "v = Visualizer(np_image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "out = v.draw_instance_predictions(predictions.to(\"cpu\"))\n",
    "# cv2_imshow(out.get_image()[:, :, ::-1])\n",
    "plt.imshow(cv2.cvtColor(out.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model\n",
    "torch.save(modelSeg, \"../models/model-seg.pth\", _use_new_zipfile_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create model asset required for Sagemaker endpoint deployment. Copy model.tar.gz to S3 bucket model folder.\n",
    "# Rename model-object.pth file name to model-object.pth per required by Sagemaker endpoint specs\n",
    "tar --transform='flags=r;s|models/model-seg.pth|model.pth|' -czvf ../models/model.tar-seg.gz ../models/model-seg.pth ../code/inference-seg.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "saved_object_model = torch.load(\"../models/model-seg.pth\")\n",
    "saved_object_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image input and get predictions\n",
    "original_image = cv2.imread(input_image) \n",
    "\n",
    "aug = T.ResizeShortestEdge(\n",
    "             [800, 800], 1333\n",
    "#             [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    "        )\n",
    "with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n",
    "        # Apply pre-processing to image.\n",
    "#         if cfg.INPUT.FORMAT == \"RGB\":\n",
    "#             # whether the model expects BGR inputs or RGB\n",
    "#             original_image = original_image[:, :, ::-1]\n",
    "        height, width = original_image.shape[:2]\n",
    "        image = aug.get_transform(original_image).apply_image(original_image)\n",
    "        image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "        inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "        predictions = saved_object_model([inputs])\n",
    "#         print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Detectron2 Visualizer on saved model ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizer for loaded model\n",
    "predictionsSeg=predictions[0][\"instances\"]\n",
    "# We can use `Visualizer` to draw the predictions on the image.\n",
    "v = Visualizer(np_image[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "out = v.draw_instance_predictions(predictionsSeg.to(\"cpu\"))\n",
    "plt.imshow(cv2.cvtColor(out.get_image()[:, :, ::-1], cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display results using custom Semantic Visualizer which does not use Detectron2 dependances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Define color map\n",
    "# ref; copied from https://github.com/facebookresearch/detectron2/blob/224cd2318fdb45b5e22bbb861ee9711ee52c8b75/detectron2/utils/colormap.py\n",
    "# RGB:\n",
    "_COLORS = np.array(\n",
    "    [\n",
    "        0.000, 0.447, 0.741,\n",
    "        0.850, 0.325, 0.098,\n",
    "        0.929, 0.694, 0.125,\n",
    "        0.494, 0.184, 0.556,\n",
    "        0.466, 0.674, 0.188,\n",
    "        0.301, 0.745, 0.933,\n",
    "        0.635, 0.078, 0.184,\n",
    "        0.300, 0.300, 0.300,\n",
    "        0.600, 0.600, 0.600,\n",
    "        1.000, 0.000, 0.000,\n",
    "        1.000, 0.500, 0.000,\n",
    "        0.749, 0.749, 0.000,\n",
    "        0.000, 1.000, 0.000,\n",
    "        0.000, 0.000, 1.000,\n",
    "        0.667, 0.000, 1.000,\n",
    "        0.333, 0.333, 0.000,\n",
    "        0.333, 0.667, 0.000,\n",
    "        0.333, 1.000, 0.000,\n",
    "        0.667, 0.333, 0.000,\n",
    "        0.667, 0.667, 0.000,\n",
    "        0.667, 1.000, 0.000,\n",
    "        1.000, 0.333, 0.000,\n",
    "        1.000, 0.667, 0.000,\n",
    "        1.000, 1.000, 0.000,\n",
    "        0.000, 0.333, 0.500,\n",
    "        0.000, 0.667, 0.500,\n",
    "        0.000, 1.000, 0.500,\n",
    "        0.333, 0.000, 0.500,\n",
    "        0.333, 0.333, 0.500,\n",
    "        0.333, 0.667, 0.500,\n",
    "        0.333, 1.000, 0.500,\n",
    "        0.667, 0.000, 0.500,\n",
    "        0.667, 0.333, 0.500,\n",
    "        0.667, 0.667, 0.500,\n",
    "        0.667, 1.000, 0.500,\n",
    "        1.000, 0.000, 0.500,\n",
    "        1.000, 0.333, 0.500,\n",
    "        1.000, 0.667, 0.500,\n",
    "        1.000, 1.000, 0.500,\n",
    "        0.000, 0.333, 1.000,\n",
    "        0.000, 0.667, 1.000,\n",
    "        0.000, 1.000, 1.000,\n",
    "        0.333, 0.000, 1.000,\n",
    "        0.333, 0.333, 1.000,\n",
    "        0.333, 0.667, 1.000,\n",
    "        0.333, 1.000, 1.000,\n",
    "        0.667, 0.000, 1.000,\n",
    "        0.667, 0.333, 1.000,\n",
    "        0.667, 0.667, 1.000,\n",
    "        0.667, 1.000, 1.000,\n",
    "        1.000, 0.000, 1.000,\n",
    "        1.000, 0.333, 1.000,\n",
    "        1.000, 0.667, 1.000,\n",
    "        0.333, 0.000, 0.000,\n",
    "        0.500, 0.000, 0.000,\n",
    "        0.667, 0.000, 0.000,\n",
    "        0.833, 0.000, 0.000,\n",
    "        1.000, 0.000, 0.000,\n",
    "        0.000, 0.167, 0.000,\n",
    "        0.000, 0.333, 0.000,\n",
    "        0.000, 0.500, 0.000,\n",
    "        0.000, 0.667, 0.000,\n",
    "        0.000, 0.833, 0.000,\n",
    "        0.000, 1.000, 0.000,\n",
    "        0.000, 0.000, 0.167,\n",
    "        0.000, 0.000, 0.333,\n",
    "        0.000, 0.000, 0.500,\n",
    "        0.000, 0.000, 0.667,\n",
    "        0.000, 0.000, 0.833,\n",
    "        0.000, 0.000, 1.000,\n",
    "        0.000, 0.000, 0.000,\n",
    "        0.143, 0.143, 0.143,\n",
    "        0.857, 0.857, 0.857,\n",
    "        1.000, 1.000, 1.000\n",
    "    ]\n",
    ").astype(np.float32).reshape(-1, 3)\n",
    "indices = random.sample(range(len(_COLORS)), 74) ## Create list of indices for random colors in the double RGB _colors array\n",
    "\n",
    "all_classed_list = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "\n",
    "ret = [[int(i) for i in (_COLORS[i] * 255)] for i in indices] ## Array containing RGB int values\n",
    "colorMap=dict(zip(all_classed_list, tuple(ret)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define custom Semantic Instance visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "with open('pan_metadata.json') as json_file:\n",
    "    metadata = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semanitc_visualizer(predictionsSegs, instance_list, image_src, thing_classes, thing_colors, boxes, opacity):\n",
    "    imageSize = np.append(tuple(reversed(image_src.size)), 4) # get image shape info\n",
    "    rgba = np.zeros(imageSize, dtype = np.uint8)\n",
    "    rgba[:, :] = [255, 255, 255, 0]\n",
    "    font = ImageFont.truetype('fonts/FreeSerif.ttf', 8)\n",
    "\n",
    "    for idx,binary_mask in enumerate(predictionsSegs):\n",
    "#         binary_mask=np.array((maskX == True),  dtype=int)\n",
    "        color=thing_colors[instance_list[idx]]\n",
    "        name=thing_classes[instance_list[idx]]\n",
    "        rgba[(binary_mask == 1), :] = np.append(color, opacity)\n",
    "\n",
    "    maskXImg = Image.fromarray(np.asarray(rgba),mode='RGBA')\n",
    "    draw = ImageDraw.Draw(maskXImg)\n",
    "#     Draw boxes and instance labels\n",
    "    for seg_info, boxx in zip(instance_list, boxes):\n",
    "      box=boxx\n",
    "      text = f\"{thing_classes[seg_info]} {.85:.0%}\"\n",
    "      len = draw.textlength(text=text)\n",
    "      bbox = draw.textbbox((box[0], box[1]), text, font=font)\n",
    "      h = bbox[3] - bbox[1]\n",
    "      draw.rectangle([(box[0], box[1]-h), (box[0] + len, box[1])], fill=(0,0,0))#text background rectangle\n",
    "      draw.text((box[0], box[1]-h), text, fill=(255, 255, 255)) # draw text on instance\n",
    "      draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=(0,255,0))#blue rectangle\n",
    "    return Image.alpha_composite(image_src, maskXImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function inputs    \n",
    "opacity=150\n",
    "predictionsSegMasks=predictions[0][\"instances\"].pred_masks.cpu() # predictions[0][\"instances\"].pred_masks.cpu().numpy()\n",
    "prediction_classes=predictions[0][\"instances\"].pred_classes\n",
    "boxes=predictions[0]['instances'].pred_boxes\n",
    "image_src = image_src.convert('RGBA')\n",
    "\n",
    "# Call function\n",
    "semanitc_visualizer(predictionsSegMasks, prediction_classes, image_src, metadata[\"thing_classes\"], metadata[\"thing_colors\"], boxes, opacity)\n",
    "# out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Semantic/Instance Detection Model to Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload model.tar.gz file to s3 bucket model folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker import get_execution_role, Session\n",
    "\n",
    "sess = Session(default_bucket='<INSERT-AWS-S3-BUCKET-NAME-HERE>')\n",
    "\n",
    "# print(model_data)\n",
    "role = get_execution_role()\n",
    "\n",
    "# Connect to S3 bucket and upload file to s3 bucket\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket('<INSERT-AWS-S3-BUCKET-NAME-HERE>').upload_file(\"../models/model.tar-seg.gz\", \"model/model.tar.gz\")\n",
    "\n",
    "uri = sess.list_s3_files(sess.default_bucket(), 'model')\n",
    "# print(uri)\n",
    "model_data = sagemaker.s3.s3_path_join('s3://', sess.default_bucket(), uri[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sematic Instance detection\n",
    "region = sess.boto_region_name\n",
    "serve_image_uri = f\"<INSERT-AWS-ELASTIC-CONTAINER-REGISTRY-REPOSITORY-NAME-HERE>\" ##custom image\n",
    "pyModel = PyTorchModel(\n",
    "    entry_point=\"inference-seg.py\",\n",
    "    source_dir=\"../code\",\n",
    "    role=role,\n",
    "    model_data=model_data,\n",
    "    image_uri=serve_image_uri,\n",
    "    framework_version=\"1.10.2\",\n",
    "    py_version=\"py38\"\n",
    ")\n",
    "\n",
    "predictorEndpt = pyModel.deploy(instance_type='ml.p3.2xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate Endpoint - perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref:\n",
    "# https://aws.amazon.com/blogs/compute/handling-binary-data-using-amazon-api-gateway-http-apis/\n",
    "# https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-payload-encodings.html\n",
    "# https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-binary-data-lambda/\n",
    "# https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-payload-encodings-configure-with-control-service-api.html\n",
    "# https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-payload-encodings-configure-with-console.html\n",
    "# https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-mapping-template-reference.html\n",
    "import boto3\n",
    "import io\n",
    "from base64 import b64encode,b64decode\n",
    "from io     import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "endpoint = '<INSERT_ENDPOINT_NAME_HERE>'\n",
    "runtime= boto3.client('runtime.sagemaker')\n",
    "\n",
    "image_src = Image.open(input_image)\n",
    "# resize image\n",
    "# size = 640, 480\n",
    "# size = 320, 240\n",
    "size = 250, 170\n",
    "# size = 160, 120\n",
    "image_src.thumbnail(size, Image.ANTIALIAS)\n",
    "    \n",
    "imgByteArr = io.BytesIO()\n",
    "\n",
    "image_src.save(imgByteArr, format=image_src.format)\n",
    "imgByteArr = imgByteArr.getvalue()\n",
    "\n",
    "# Send image via InvokeEndpoint API\n",
    "response = runtime.invoke_endpoint(EndpointName=endpoint, ContentType='application/x-image', Body=imgByteArr)\n",
    "result = response['Body'].read().decode()\n",
    "res = json.loads(result) # convert json string to Python dict for parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display Detectron2 Semantic Instance segmentation inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function inputs    \n",
    "opacity=150\n",
    "predictionsSegMasks=np.array(res[0]['sematic_seg']) # res[0]['sematic_seg']\n",
    "prediction_classes=res[0][\"pred_classes\"]\n",
    "boxes=res[0]['pred_boxes']\n",
    "\n",
    "image_src = image_src.convert('RGBA')\n",
    "\n",
    "# Call function\n",
    "semanitc_visualizer(predictionsSegMasks, prediction_classes, image_src, metadata[\"thing_classes\"], metadata[\"thing_colors\"], boxes, opacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Semantic Instance API using Python Request library\n",
    "\n",
    "Create AWS API gateway before performing this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some common libraries\n",
    "# Using Python Request library\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import io\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Define Constants\n",
    "API_INVOKE_URL=\"<INSERT_API_INVOKE_URL_HERE>\"\n",
    "\n",
    "# define variables\n",
    "url=API_INVOKE_URL\n",
    "\n",
    "def cloud_api_predict(headers, payload):\n",
    "    # send POST request to url\n",
    "    return requests.request(\"POST\", url, headers=headers, data=payload).text\n",
    "\n",
    "# Read image into memory - needed because of image size reduction\n",
    "image_src = Image.open(input_image)\n",
    "# resize image\n",
    "# size = 640, 480\n",
    "# size = 320, 240\n",
    "size = 250, 170\n",
    "# size = 160, 120\n",
    "image_src.thumbnail(size, Image.ANTIALIAS)\n",
    "\n",
    "imgByteArr = io.BytesIO()\n",
    "\n",
    "image_src.save(imgByteArr, format=image_src.format)\n",
    "payload = imgByteArr.getvalue()\n",
    "\n",
    "# with open(input_image, 'rb') as f:\n",
    "#     payload = f.read()\n",
    "\n",
    "headers = {\n",
    "  'Accept': 'image/jpeg',\n",
    "  'Content-Type': 'image/jpeg'\n",
    "}\n",
    "\n",
    "predictions=cloud_api_predict(headers, payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display Detectron2 Object Detections inference results from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = json.loads(predictions) # convert json string to Python dict for parsing\n",
    "# Define function inputs    \n",
    "opacity=150\n",
    "predictionsSegMasks=np.array(res[0]['sematic_seg']) # res[0]['sematic_seg']\n",
    "prediction_classes=res[0][\"pred_classes\"]\n",
    "boxes=res[0]['pred_boxes']\n",
    "\n",
    "image_src = image_src.convert('RGBA')\n",
    "\n",
    "# Call function\n",
    "semanitc_visualizer(predictionsSegMasks, prediction_classes, image_src, metadata[\"thing_classes\"], metadata[\"thing_colors\"], boxes, opacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create aws Lamda deployment zip package\n",
    "This is required because Pillow is not automatically included in AWS lamda environment.\n",
    "#Ref:\n",
    "\n",
    "https://docs.aws.amazon.com/lambda/latest/dg/lambda-deploy-functions.html\n",
    "\n",
    "https://docs.aws.amazon.com/lambda/latest/dg/python-package.html#python-package-create-package-no-dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda template\n",
    "!pip install virtualenv\n",
    "# !ls -l\n",
    "%cd lamdaenv\n",
    "# !pwd\n",
    "!python3 -m venv myvenv\n",
    "!source myvenv/bin/activate\n",
    "!pip install Pillow\n",
    "!deactivate\n",
    "%cd ..\n",
    "# !ls -l myvenv/lib/python3.6/site-packages\n",
    "# !ls -l myvenv/lib64/python3.6/site-packages\n",
    "%cd myvenv/lib/python3.6/site-packages\n",
    "!zip -r ../../../../my-deployment-package1.zip .\n",
    "%cd ../../../../\n",
    "\n",
    "%cd myvenv/lib64/python3.6/site-packages\n",
    "!zip -g my-deployment-packag1e.zip lambda_function.py\n",
    "# !ls -l\n",
    "# !pwd\n",
    "# %cd SageMaker\n",
    "\n",
    "# !pip show Pillow | grep Location:\n",
    "# %cd /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages\n",
    "# !zip -r ../../../../../../SageMaker/lamdaenv/my-deployment-package.zip PIL Pillow-8.4.0.dist-info Pillow.libs\n",
    "# %cd ../../../../../../SageMaker/lamdaenv\n",
    "# !zip -g my-deployment-package.zip lambda_function.py\n",
    "\n",
    "# !pwd\n",
    "# !ls -l\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Detectron2 Tutorial Demo.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
